{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd742ae",
   "metadata": {},
   "source": [
    "# Unzip the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93bc9e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"MaskedUnmaskedDataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"MaskedUnmaskedDataset\")\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29bc32",
   "metadata": {},
   "source": [
    "# Build a Convolutional Neural Network (CNN) for Face Mask Classification\n",
    "\n",
    "We will now build and implement a CNN model to classify images as masked, unmasked, or partially masked faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcc8c5",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6184284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import required libraries for PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c43a2",
   "metadata": {},
   "source": [
    "Peforms Data Augmentation and prepares dataset to train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21ed4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "# Set up data transforms and load the dataset with advanced data augmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Advanced data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Simpler transform for validation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset_dir = 'MaskedUnmaskedDataset'  # Path to extracted dataset\n",
    "\n",
    "dataset = ImageFolder(root=dataset_dir, transform=train_transform)\n",
    "\n",
    "# Split dataset into train and val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Apply correct transforms to train/val splits\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Show class names\n",
    "display_classes = dataset.classes\n",
    "print('Classes:', display_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b11fdb",
   "metadata": {},
   "source": [
    "# Explanation of CNN Model Parameters and Layers\n",
    "\n",
    "**Model Architecture:**\n",
    "- The model is a simple Convolutional Neural Network (CNN) designed for image classification.\n",
    "\n",
    "**Layers and Parameters:**\n",
    "- `nn.Conv2d(3, 32, kernel_size=3, padding=1)`: First convolutional layer. Takes 3-channel (RGB) images as input, outputs 32 feature maps, uses a 3x3 kernel, and padding of 1 to preserve spatial dimensions.\n",
    "- `nn.ReLU()`: Activation function introducing non-linearity.\n",
    "- `nn.MaxPool2d(2, 2)`: Downsamples the feature maps by a factor of 2 (2x2 window).\n",
    "- `nn.Conv2d(32, 64, kernel_size=3, padding=1)`: Second convolutional layer. Takes 32 input channels, outputs 64 feature maps.\n",
    "- `nn.ReLU()` and `nn.MaxPool2d(2, 2)`: As above.\n",
    "- `nn.Conv2d(64, 128, kernel_size=3, padding=1)`: Third convolutional layer. Takes 64 input channels, outputs 128 feature maps.\n",
    "- `nn.ReLU()` and `nn.MaxPool2d(2, 2)`: As above.\n",
    "- `nn.Flatten()`: Flattens the output from the convolutional layers into a 1D vector for the fully connected layers.\n",
    "- `nn.Linear(128 * 16 * 16, 128)`: Fully connected layer. Input size is 128 feature maps of size 16x16 (after pooling), output is 128 features.\n",
    "- `nn.ReLU()`: Activation function.\n",
    "- `nn.Dropout(0.5)`: Randomly sets 50% of the input units to 0 during training to help prevent overfitting.\n",
    "- `nn.Linear(128, num_classes)`: Final fully connected layer. Outputs a vector with length equal to the number of classes (masked, unmasked, partially masked).\n",
    "\n",
    "**Other Parameters:**\n",
    "- `num_classes`: The number of output classes, determined from the dataset.\n",
    "\n",
    "This architecture is a good starting point for image classification tasks and can be further tuned for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad878d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceMaskCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU()\n",
      "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (23): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=32768, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Improved custom CNN model for face mask classification\n",
    "class FaceMaskCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FaceMaskCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(display_classes)\n",
    "model = FaceMaskCNN(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ac98b",
   "metadata": {},
   "source": [
    "# Training the CNN Model\n",
    "\n",
    "This code trains the custom FaceMaskCNN model using CrossEntropyLoss and Adam optimizer (lr=0.0005) \n",
    "for 3 epochs. It performs training and validation in each epoch, prints metrics, and saves the model \n",
    "with the best validation accuracy to 'best_facemaskcnn.pth'. The model runs on GPU if available.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926587cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]: 100%|██████████| 96/96 [01:57<00:00,  1.22s/it]\n",
      "Epoch 1/3 [Val]: 100%|██████████| 24/24 [00:15<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Train Loss: 0.1883 | Train Acc: 0.9410 | Val Loss: 0.1181 | Val Acc: 0.9596\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|██████████| 96/96 [01:28<00:00,  1.09it/s]\n",
      "Epoch 2/3 [Val]: 100%|██████████| 24/24 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Train Loss: 0.1470 | Train Acc: 0.9478 | Val Loss: 0.1422 | Val Acc: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|██████████| 96/96 [02:08<00:00,  1.34s/it]\n",
      "Epoch 3/3 [Val]: 100%|██████████| 24/24 [00:08<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Train Loss: 0.1392 | Train Acc: 0.9517 | Val Loss: 0.0987 | Val Acc: 0.9700\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the improved FaceMaskCNN model\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 3\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_facemaskcnn.pth')\n",
    "        print(\"Best model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a79e6",
   "metadata": {},
   "source": [
    "# Face Detection and Mask Classification with YOLOv8-Face\n",
    "\n",
    "This cell uses the YOLOv5-face model for accurate face detection and a ResNet18-based classifier for mask classification. Detected faces are shown with bounding boxes and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ad43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "C:\\Users\\Aarmaan Choudhary\\AppData\\Local\\Temp\\ipykernel_3208\\286518444.py:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "  yolo_model = YOLO('yolov8-face\\yolov8n-face.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 faces, 129.3ms\n",
      "Speed: 4.3ms preprocess, 129.3ms inference, 39.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Image with face rectangles saved as 'output_yolov8_face_rect.jpg'\n",
      "0: 384x640 2 faces, 129.3ms\n",
      "Speed: 4.3ms preprocess, 129.3ms inference, 39.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Image with face rectangles saved as 'output_yolov8_face_rect.jpg'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load standard YOLO model\n",
    "yolo_model = YOLO('yolov8-face\\yolov8n-face.pt')\n",
    "\n",
    "img = cv2.imread('images.jpeg')\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run detection\n",
    "results = yolo_model(img)\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imwrite('output_yolov8_face_rect.jpg', img)\n",
    "print(\"Image with face rectangles saved as 'output_yolov8_face_rect.jpg'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344fe5fd",
   "metadata": {},
   "source": [
    "# Masked and Unmasked Classification using YOLOV5 and custom CNN\n",
    "\n",
    "The folloeing code block uses the yolov8-face model to recognise faces from images, the rectangular selection around the face is selected and sent to the CNN after being converted to a tensor, where it classifies the image as masked or unmasked. The result is added along with a box around the faces of people in the image.It is then saved as a seperate image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe396bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 faces, 101.0ms\n",
      "Speed: 2.2ms preprocess, 101.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0: 384x640 2 faces, 101.0ms\n",
      "Speed: 2.2ms preprocess, 101.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Image with mask labels saved as 'output_yolov8_face_masked_unmasked.jpg'\n",
      "Image with mask labels saved as 'output_yolov8_face_masked_unmasked.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Use YOLOv8-face detections and FaceMaskCNN to label faces as masked or unmasked\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "model = FaceMaskCNN(num_classes)\n",
    "model.load_state_dict(torch.load('best_facemaskcnn.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Use the same image as before\n",
    "img = cv2.imread('images.jpeg')\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run YOLOv8-face detection\n",
    "results = yolo_model(img_rgb)\n",
    "\n",
    "# Use the same transform as used for training the CNN\n",
    "face_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            face_img = img[y1:y2, x1:x2]\n",
    "            if face_img.size == 0:\n",
    "                continue\n",
    "            try:\n",
    "                face_tensor = face_transform(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)).unsqueeze(0)\n",
    "            except Exception as e:\n",
    "                print(\"Transform error:\", e)\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                outputs = model(face_tensor)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                label = display_classes[pred.item()]\n",
    "            color = (0, 255, 0) if label == 'with_mask' or label == 'masked' else (0, 0, 255)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 2)  \n",
    "\n",
    "cv2.imwrite('output_yolov8_face_masked_unmasked.jpg', img)\n",
    "print(\"Image with mask labels saved as 'output_yolov8_face_masked_unmasked.jpg'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
