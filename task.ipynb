{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643bfa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 293 images belonging to 3 classes.\n",
      "Found 97 images belonging to 3 classes.\n",
      "Found 39 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.6502 - loss: 1.1851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 486ms/step - accuracy: 0.6587 - loss: 1.1592 - val_accuracy: 0.8557 - val_loss: 0.5138\n",
      "Epoch 2/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 286ms/step - accuracy: 0.8634 - loss: 0.5229 - val_accuracy: 0.9175 - val_loss: 0.3215\n",
      "Epoch 3/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 275ms/step - accuracy: 0.9013 - loss: 0.4564 - val_accuracy: 0.9588 - val_loss: 0.2470\n",
      "Epoch 4/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 275ms/step - accuracy: 0.9118 - loss: 0.4108 - val_accuracy: 0.9588 - val_loss: 0.2058\n",
      "Epoch 5/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 271ms/step - accuracy: 0.9086 - loss: 0.3167 - val_accuracy: 0.9588 - val_loss: 0.1833\n",
      "Epoch 6/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 282ms/step - accuracy: 0.9509 - loss: 0.1750 - val_accuracy: 0.9588 - val_loss: 0.1798\n",
      "Epoch 7/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 297ms/step - accuracy: 0.9601 - loss: 0.1838 - val_accuracy: 0.9588 - val_loss: 0.1764\n",
      "Epoch 8/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 281ms/step - accuracy: 0.9548 - loss: 0.1832 - val_accuracy: 0.9691 - val_loss: 0.1504\n",
      "Epoch 9/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 275ms/step - accuracy: 0.9125 - loss: 0.2443 - val_accuracy: 0.9588 - val_loss: 0.1389\n",
      "Epoch 10/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 281ms/step - accuracy: 0.9518 - loss: 0.1440 - val_accuracy: 0.9691 - val_loss: 0.1421\n",
      "Epoch 11/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 280ms/step - accuracy: 0.9461 - loss: 0.1804 - val_accuracy: 0.9691 - val_loss: 0.1354\n",
      "Epoch 12/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 303ms/step - accuracy: 0.9494 - loss: 0.1428 - val_accuracy: 0.9691 - val_loss: 0.1282\n",
      "Epoch 13/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 279ms/step - accuracy: 0.9723 - loss: 0.0891 - val_accuracy: 0.9588 - val_loss: 0.1108\n",
      "Epoch 14/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 269ms/step - accuracy: 0.9591 - loss: 0.1112 - val_accuracy: 0.9588 - val_loss: 0.1113\n",
      "Epoch 15/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 288ms/step - accuracy: 0.9780 - loss: 0.0787 - val_accuracy: 0.9691 - val_loss: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model trained and saved as mask_model_mobilenetv2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 926ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\mask_weared_incorrect_5.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\mask_weared_incorrect_6.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_103.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_120.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_136.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_14.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_149.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_15.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_151.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_152.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_155.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_164.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_166.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_182.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_192.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_204.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_221.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_228.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_24.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_240.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_254.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_256.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_261.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_263.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_282.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_287.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_297.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_33.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_45.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_48.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_67.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_79.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_80.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\with_mask_82.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\without_mask_14.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\without_mask_2.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\without_mask_31.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\without_mask_36.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "âœ… Saved labeled image: labeled_test_images\\without_mask_7.jpg\n",
      "ðŸŽ‰ Done labeling test images!\n"
     ]
    }
   ],
   "source": [
    "# transfer_learning_mask_detector.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Dataset directories ===\n",
    "base_data_dir = 'faces'\n",
    "train_dir = os.path.join(base_data_dir, 'train')\n",
    "val_dir = os.path.join(base_data_dir, 'val')\n",
    "test_dir = os.path.join(base_data_dir, 'test')\n",
    "\n",
    "# === Parameters ===\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "# === Data generators ===\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_data = val_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# === Load base model ===\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train model ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "model.save('mask_model_mobilenetv2.h5')\n",
    "print(\"\\nâœ… Model trained and saved as mask_model_mobilenetv2.h5\")\n",
    "\n",
    "# === Predict and label test images ===\n",
    "output_dir = \"labeled_test_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "model = load_model(\"mask_model_mobilenetv2.h5\")\n",
    "\n",
    "for i in range(len(test_data.filenames)):\n",
    "    img_path = os.path.join(test_dir, test_data.filenames[i])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    if original_image is None:\n",
    "        continue\n",
    "\n",
    "    image = cv2.resize(original_image, img_size)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    pred = model.predict(image)[0]\n",
    "    class_id = np.argmax(pred)\n",
    "    label = class_names[class_id]\n",
    "    confidence = pred[class_id]\n",
    "\n",
    "    label_text = f\"{label} ({confidence:.2f})\"\n",
    "    (text_w, text_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "    cv2.rectangle(original_image, (5, 5), (5 + text_w + 10, 5 + text_h + 20), (0, 0, 0), -1)\n",
    "    cv2.putText(original_image, label_text, (10, 10 + text_h), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    save_path = os.path.join(output_dir, os.path.basename(img_path))\n",
    "    cv2.imwrite(save_path, original_image)\n",
    "    print(f\"âœ… Saved labeled image: {save_path}\")\n",
    "\n",
    "print(\"ðŸŽ‰ Done labeling test images!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a712db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Found 2 face(s) in images_2.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "âœ… Saved result: output\\images_2.jpeg\n",
      "ðŸ” Found 3 face(s) in images_4.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "âœ… Saved result: output\\images_4.jpeg\n",
      "ðŸ” Found 2 face(s) in image_0.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "âœ… Saved result: output\\image_0.jpeg\n",
      "ðŸ” Found 1 face(s) in image_1.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "âœ… Saved result: output\\image_1.jpeg\n",
      "ðŸ” Found 0 face(s) in image_3.jpeg\n",
      "âœ… Saved result: output\\image_3.jpeg\n",
      "ðŸ” Found 1 face(s) in image_5.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "âœ… Saved result: output\\image_5.jpeg\n",
      "ðŸ” Found 0 face(s) in image_6.jpeg\n",
      "âœ… Saved result: output\\image_6.jpeg\n",
      "ðŸ” Found 0 face(s) in image_7.jpeg\n",
      "âœ… Saved result: output\\image_7.jpeg\n",
      "ðŸ” Found 0 face(s) in mask_weared_incorrect_5.jpg\n",
      "âœ… Saved result: output\\mask_weared_incorrect_5.jpg\n",
      "ðŸ” Found 0 face(s) in mask_weared_incorrect_6.jpg\n",
      "âœ… Saved result: output\\mask_weared_incorrect_6.jpg\n",
      "ðŸ” Found 0 face(s) in without_mask_14.jpg\n",
      "âœ… Saved result: output\\without_mask_14.jpg\n",
      "ðŸ” Found 0 face(s) in without_mask_2.jpg\n",
      "âœ… Saved result: output\\without_mask_2.jpg\n",
      "ðŸ” Found 0 face(s) in without_mask_31.jpg\n",
      "âœ… Saved result: output\\without_mask_31.jpg\n",
      "ðŸ” Found 0 face(s) in without_mask_36.jpg\n",
      "âœ… Saved result: output\\without_mask_36.jpg\n",
      "ðŸ” Found 0 face(s) in without_mask_7.jpg\n",
      "âœ… Saved result: output\\without_mask_7.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_103.jpg\n",
      "âœ… Saved result: output\\with_mask_103.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_120.jpg\n",
      "âœ… Saved result: output\\with_mask_120.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_136.jpg\n",
      "âœ… Saved result: output\\with_mask_136.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_14.jpg\n",
      "âœ… Saved result: output\\with_mask_14.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_149.jpg\n",
      "âœ… Saved result: output\\with_mask_149.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_15.jpg\n",
      "âœ… Saved result: output\\with_mask_15.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_151.jpg\n",
      "âœ… Saved result: output\\with_mask_151.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_152.jpg\n",
      "âœ… Saved result: output\\with_mask_152.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_155.jpg\n",
      "âœ… Saved result: output\\with_mask_155.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_164.jpg\n",
      "âœ… Saved result: output\\with_mask_164.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_166.jpg\n",
      "âœ… Saved result: output\\with_mask_166.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_182.jpg\n",
      "âœ… Saved result: output\\with_mask_182.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_192.jpg\n",
      "âœ… Saved result: output\\with_mask_192.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_204.jpg\n",
      "âœ… Saved result: output\\with_mask_204.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_221.jpg\n",
      "âœ… Saved result: output\\with_mask_221.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_228.jpg\n",
      "âœ… Saved result: output\\with_mask_228.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_24.jpg\n",
      "âœ… Saved result: output\\with_mask_24.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_240.jpg\n",
      "âœ… Saved result: output\\with_mask_240.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_254.jpg\n",
      "âœ… Saved result: output\\with_mask_254.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_256.jpg\n",
      "âœ… Saved result: output\\with_mask_256.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_261.jpg\n",
      "âœ… Saved result: output\\with_mask_261.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_263.jpg\n",
      "âœ… Saved result: output\\with_mask_263.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_282.jpg\n",
      "âœ… Saved result: output\\with_mask_282.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_287.jpg\n",
      "âœ… Saved result: output\\with_mask_287.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_297.jpg\n",
      "âœ… Saved result: output\\with_mask_297.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_33.jpg\n",
      "âœ… Saved result: output\\with_mask_33.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_45.jpg\n",
      "âœ… Saved result: output\\with_mask_45.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_48.jpg\n",
      "âœ… Saved result: output\\with_mask_48.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_67.jpg\n",
      "âœ… Saved result: output\\with_mask_67.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_79.jpg\n",
      "âœ… Saved result: output\\with_mask_79.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_80.jpg\n",
      "âœ… Saved result: output\\with_mask_80.jpg\n",
      "ðŸ” Found 0 face(s) in with_mask_82.jpg\n",
      "âœ… Saved result: output\\with_mask_82.jpg\n",
      "ðŸŽ‰ Done. All labeled images saved in 'output/' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load the trained classification model\n",
    "model = load_model(\"mask_model_mobilenetv2.h5\")  # or \"mask_model.keras\"\n",
    "\n",
    "# Class names in training order\n",
    "class_names = ['mask_weared_incorrect', 'with_mask', 'without_mask']\n",
    "\n",
    "# Load OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Input/output directories\n",
    "input_folder = \"test_images\"\n",
    "output_folder = \"output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each image\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        print(f\"âŒ Could not read {filename}\")\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    print(f\"ðŸ” Found {len(faces)} face(s) in {filename}\")\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = image[y:y + h, x:x + w]\n",
    "        if face.size == 0:\n",
    "            continue\n",
    "\n",
    "        face_resized = cv2.resize(face, (128, 128))\n",
    "        face_normalized = face_resized.astype(\"float32\") / 255.0\n",
    "        face_input = img_to_array(face_normalized)\n",
    "        face_input = np.expand_dims(face_input, axis=0)\n",
    "\n",
    "        pred = model.predict(face_input)[0]\n",
    "        class_id = np.argmax(pred)\n",
    "        label = class_names[class_id]\n",
    "        confidence = pred[class_id]\n",
    "\n",
    "        color = (0, 255, 0) if label == \"with_mask\" else (0, 0, 255)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        label_text = f\"{label} ({confidence:.2f})\"\n",
    "        (text_w, text_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        label_y = y - 10 if y - 10 > text_h else y + h + 20\n",
    "        cv2.rectangle(image, (x, label_y - text_h - 4), (x + text_w, label_y + 4), color, -1)\n",
    "        cv2.putText(image, label_text, (x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    cv2.imwrite(save_path, image)\n",
    "    print(f\"âœ… Saved result: {save_path}\")\n",
    "\n",
    "print(\"ðŸŽ‰ Done. All labeled images saved in 'output/' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
